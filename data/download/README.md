This document describes how to use the download and data
cleaning processes implemented to scrape audio from the 
downloaded videos, upload that audio as wavs to a google
cloud bucket, run a long transcription on that bucket, 
and then extract the transcription into a separate cloud bucket
that can be used to match the word timings with gesture 
timings. 

### Downloading speaker data

first we must download all speaker videos. 
```buildoutcfg
python -m data.download.download_youtube --base_path </path/to/dataset base folder> --speaker <speaker_name>
```

This creates the following directory structure
```buildoutcfg
Gestures
├── frames.csv
├── almaram
    ├── frames
    ├── videos
    ├── keypoints_all
    ├── keypoints_simple
    └── videos
...
└── shelly
    ├── frames
    ├── videos
    ├── keypoints_all
    ├── keypoints_simple
    └── videos
```

Then, after downloading [the intervals](https://drive.google.com/drive/folders/1qvvnfGwas8DUBrwD4DoBnvj8anjSLldZ) dataframe, 
to create the intervals DF of gestures that is specific to each speaker, and which will
be input to the semantic-text-gesture matcher, run: 
```buildoutcfg
python -m data.download.crop_intervals --base_path </path/to/base folder> --speaker <optional, speaker_name> --output_path <output path to save intervals>
```
Where we want the interval path to be the videos path of the speaker. 

This creates `intervals_df.csv` unique to each speaker? 

However, the data we want to match to the training is `train.csv`, generated by the command
```buildoutcfg
python -m data.train_test_data_extraction.extract_data_for_training --base_dataset_path <base_path> --speaker <speaker_name> -np <number of processes> --speaker <speaker name>`
```
`train.csv` is a long csv in which each sample is a few seconds long. This contains all of the **motion** information
that will be used to train the net. In order to add the raw text and semantic information, 
follow the procedure below: 

### Extracting audio data
Once the youtube videos have been downloaded using the above command, you can run
```buildoutcfg
python -m data.download.upload_audio_from_videos --base_dataset_path Gestures/ --speaker <speaker name> --output_path tmp_audio_output --scrape --upload
```
This currently uploads audio to `audio_bucket_rock_1` as a mono wav format (which is necessary to do
a long-running transcription in google cloud).

From here, to transcribe the audio from that bucket, and store the resulting csv of the transcription in google cloud run
```buildoutcfg
python -m data.download.transcribe_audio_in_cloud --csv_output_path tmp_csv_files --audio_path tmp_audio_output
```
**Importantly** the `audio_path` that is passed to `transcribe_audio_in_cloud` must be a directory which contains
the audio files that are named identically to those uploaded by the `upload_audio_from_videos` script. It is easiest
to do this by running one after the other, and with the same path being passed as `audio_output_path` and `audio_path`.

These currently upload audio to `audio_bucket_rock_1` and csv to `audio_transcript_bucket_1`


#### For older transcriptions
Because I was a big dumb dummy when I did the last round of transcriptions, we need to convert 
older transcripts to be the new csv format. This was hopefully a one-time process, 
but worth documenting here for posterity. 
```
python -m data.download.convert_json_transcripts_to_csv 
```
This just took all the json transcripts sitting in the transcript bucket and converted them to 
word csvs so that the information can be matched more easily. 


### And they all lived happily ever after.
After you have extracted `train.csv` and have all audio data for a speaker uploaded to a 
cloud bucket, run
```
python -m data.download.text_gesture_interval_matcher --train_csv <path to train csv> --base_path <base path to TRAINING data>
```
This searches the transcript bucket for filenames that match the `video_fn` 
in the training csv, then adds the words and semantic analysis of those words based 
on matching the timestamps of the gestures extracted for training. The output is stored in 
`train_wioth_text.csv` along the base path.

Finally, to add semantic analysis, run:
```
python -m data.word_processing.add_semantic_analysis_to_train --base_path Gestures/ --train_csv <path to train_with_text.csv>
```
this adds the semantic analysis, including
* raw text vectorization
* wordnet relevant categories

##### TODO:
* make scrape, upload, and transcription scripts take bucket name as input.
* pull out google cloud helpers.
* Delete all `.json` files from cloud transcripts!!!!